{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv3D, Flatten,MaxPooling3D,AveragePooling3D, concatenate,Input ,SpatialDropout3D,Dropout\n",
    "import keras\n",
    "from math import e\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = xr.open_dataset('../../Feb2021/Final_Models/Teleconnections/CNN_input.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2mTsAll=inputData.t2mTsAll\n",
    "wsTsAll=inputData.wsTsAll\n",
    "rhTsAll=inputData.rhTsAll\n",
    "invTsAll=inputData.invTsAll\n",
    "wTsAll=inputData.wTsAll\n",
    "ushearTsAll=inputData.ushearTsAll\n",
    "AO5DAll=inputData.AO5DAll\n",
    "EU5DAll=inputData.EU5DAll\n",
    "SST30DAll=inputData.SST30DAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "1086\n",
      "745\n",
      "382\n",
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.06461952, 0.56141439, 0.38513234, 0.19747725, 0.04135649]),\n",
       " array([0. , 0.8, 1.6, 2.4, 3.2, 4. ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fogData = xr.open_dataset('../../../Data/FogData/CombinedFogData_25Stations.nc')\n",
    "#plt.figure(figsize=[16,8])\n",
    "StackFog=fogData.fogdata.stack(a=('years','months','days'))\n",
    "StackFog\n",
    "dd =[];\n",
    "for i in range(StackFog.years.values.shape[0]):\n",
    "    dd=dd+[str(StackFog.years[i].values)+'-'+str(StackFog.months[i].values)+\"-\"+str(StackFog.days[i].values)]\n",
    "fg = xr.Dataset({'fogdata': (('time','stations'), StackFog.values.T)}, coords={'time': pd.to_datetime(dd),'stations': fogData.stations})\n",
    "yAll=fg.fogdata.sum(dim='stations').sel(time=slice('1980-1-1','2018-12-31'))\n",
    "yAll=yAll\n",
    "yAll[(yAll>0)&(yAll<=6)]=1\n",
    "yAll[(yAll>5)&(yAll<=12)]=2\n",
    "yAll[(yAll>12)&(yAll<=18)]=3\n",
    "yAll[(yAll>18)]=4\n",
    "print(yAll[yAll==0].shape[0])\n",
    "print(yAll[yAll==1].shape[0])\n",
    "print(yAll[yAll==2].shape[0])\n",
    "print(yAll[yAll==3].shape[0])\n",
    "print(yAll[yAll==4].shape[0])\n",
    "plt.hist(yAll, bins=5, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_train,t2m_test,ws_train,ws_test,rh_train,rh_test,inv_train,inv_test, w_train, w_test,ushear_train, ushear_test,AO5D_train,AO5D_test,EU5D_train,EU5D_test,SST30D_train,SST30D_test,y_train,y_test= train_test_split( t2mTsAll,wsTsAll,rhTsAll,invTsAll, wTsAll,ushearTsAll,AO5DAll,EU5DAll,SST30DAll,yAll,test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1934, 18, 26, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2mtrain=t2m_train.values\n",
    "t2mtrain=t2mtrain[:,:,:,None]\n",
    "t2mtrain.shape\n",
    "\n",
    "\n",
    "wstrain=ws_train.values\n",
    "wstrain=wstrain[:,:,:,None]\n",
    "wstrain.shape\n",
    "\n",
    "rhtrain=rh_train.values\n",
    "rhtrain=rhtrain[:,:,:,None]\n",
    "rhtrain.shape\n",
    "\n",
    "\n",
    "invtrain=inv_train.values\n",
    "invtrain=invtrain[:,:,:,None]\n",
    "invtrain.shape\n",
    "\n",
    "wtrain=w_train.values\n",
    "wtrain=wtrain[:,:,:,None]\n",
    "wtrain.shape\n",
    "\n",
    "usheartrain=ushear_train.values\n",
    "usheartrain=usheartrain[:,:,:,None]\n",
    "usheartrain.shape\n",
    "\n",
    "AO5Dtrain=AO5D_train.values\n",
    "AO5Dtrain=AO5Dtrain[:,:,:,None]\n",
    "AO5Dtrain.shape\n",
    "\n",
    "EU5Dtrain=EU5D_train.values\n",
    "EU5Dtrain=EU5Dtrain[:,:,:,None]\n",
    "EU5Dtrain.shape\n",
    "\n",
    "SST30Dtrain=SST30D_train.values\n",
    "SST30Dtrain=SST30Dtrain[:,:,:,None]\n",
    "SST30Dtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 18, 26, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2mtest=t2m_test.values\n",
    "t2mtest=t2mtest[:,:,:,None]\n",
    "t2mtest.shape\n",
    "\n",
    "\n",
    "wstest=ws_test.values\n",
    "wstest=wstest[:,:,:,None]\n",
    "wstest.shape\n",
    "\n",
    "rhtest=rh_test.values\n",
    "rhtest=rhtest[:,:,:,None]\n",
    "rhtest.shape\n",
    "\n",
    "\n",
    "invtest=inv_test.values\n",
    "invtest=invtest[:,:,:,None]\n",
    "invtest.shape\n",
    "\n",
    "wtest=w_test.values\n",
    "wtest=wtest[:,:,:,None]\n",
    "wtest.shape\n",
    "\n",
    "usheartest=ushear_test.values\n",
    "usheartest=usheartest[:,:,:,None]\n",
    "usheartest.shape\n",
    "\n",
    "AO5Dtest=AO5D_test.values\n",
    "AO5Dtest=AO5Dtest[:,:,:,None]\n",
    "AO5Dtest.shape\n",
    "\n",
    "EU5Dtest=EU5D_test.values\n",
    "EU5Dtest=EU5Dtest[:,:,:,None]\n",
    "EU5Dtest.shape\n",
    "\n",
    "SST30Dtest=SST30D_test.values\n",
    "SST30Dtest=SST30Dtest[:,:,:,None]\n",
    "SST30Dtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain=y_train.values\n",
    "ytrain=ytrain[:,None]\n",
    "ytrain.shape\n",
    "ytest=y_test.values\n",
    "ytest=ytest[:,None]\n",
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding of labels\n",
    "from keras.utils import np_utils\n",
    "y_train=np_utils.to_categorical(y_train,5)\n",
    "y_test=np_utils.to_categorical(y_test,5)\n",
    "\n",
    "print(y_train[1]) # output [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] which means label is 6 which represents a frog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1934, 18, 26, 1)\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_37 (Conv3D)           (None, 6, 18, 26, 16)     448       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_24 (Averag (None, 3, 9, 13, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 3, 9, 13, 32)      13856     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_25 (Averag (None, 2, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 2, 5, 7, 64)       55360     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_26 (Averag (None, 1, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 1, 3, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_27 (Averag (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 326,053\n",
      "Trainable params: 326,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 2s 18ms/step - loss: 1.3862 - acc: 0.4084\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.3028 - acc: 0.4487\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.2837 - acc: 0.4489\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.3190 - acc: 0.4148\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.2663 - acc: 0.4399\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.2576 - acc: 0.4428\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 1.1810 - acc: 0.4820\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 1.0798 - acc: 0.5384\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.9789 - acc: 0.5850\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.9765 - acc: 0.5834\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.9005 - acc: 0.6169\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.8800 - acc: 0.6249\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.8540 - acc: 0.6221\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.8326 - acc: 0.6348\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.8516 - acc: 0.6260\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.8057 - acc: 0.6596\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7552 - acc: 0.6634\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7439 - acc: 0.6732\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7348 - acc: 0.6900\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7672 - acc: 0.6372\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.7299 - acc: 0.6777\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6850 - acc: 0.7120\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6201 - acc: 0.7374\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6307 - acc: 0.7282\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.6087 - acc: 0.7277\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.6226 - acc: 0.7378\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.5762 - acc: 0.7505\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.5336 - acc: 0.7787\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.5649 - acc: 0.7646\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.5003 - acc: 0.7919\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.4914 - acc: 0.7944\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.4737 - acc: 0.8095\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.4476 - acc: 0.8201\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.4330 - acc: 0.8158\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.3828 - acc: 0.8456\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.4379 - acc: 0.8292\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.3564 - acc: 0.8515\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.3086 - acc: 0.8724\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.3301 - acc: 0.8723\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2887 - acc: 0.8819\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.2939 - acc: 0.8806\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2578 - acc: 0.9024\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2719 - acc: 0.8860\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.2786 - acc: 0.8902\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.2665 - acc: 0.8994\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2073 - acc: 0.9221\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2259 - acc: 0.9078\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2532 - acc: 0.9049\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2313 - acc: 0.9055\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.2103 - acc: 0.9115\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1880 - acc: 0.9300\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1697 - acc: 0.9318\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1634 - acc: 0.9433\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1724 - acc: 0.9265\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1720 - acc: 0.9378\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.1664 - acc: 0.9333\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1203 - acc: 0.9532\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.1571 - acc: 0.9397\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1382 - acc: 0.9448\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.1261 - acc: 0.9596\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1319 - acc: 0.9450\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1398 - acc: 0.9498\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1374 - acc: 0.9459\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1222 - acc: 0.9589\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1427 - acc: 0.9417\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1306 - acc: 0.9566\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1028 - acc: 0.9570\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0905 - acc: 0.9704\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0879 - acc: 0.9713\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.1301 - acc: 0.9469\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0853 - acc: 0.9657\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0776 - acc: 0.9728\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1027 - acc: 0.9688\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0917 - acc: 0.9663\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0954 - acc: 0.9706\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.1296 - acc: 0.9514\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 1s 19ms/step - loss: 0.0769 - acc: 0.9753\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1121 - acc: 0.9542\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0884 - acc: 0.9650\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0952 - acc: 0.9700\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0907 - acc: 0.9628\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0790 - acc: 0.9732\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0873 - acc: 0.9694\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0984 - acc: 0.9677\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0709 - acc: 0.9752\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0754 - acc: 0.9718\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0787 - acc: 0.9753\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.1129 - acc: 0.9630\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0803 - acc: 0.9747\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0712 - acc: 0.9725\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0836 - acc: 0.9747\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0589 - acc: 0.9786\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0709 - acc: 0.9764\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0768 - acc: 0.9726\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0875 - acc: 0.9616\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0647 - acc: 0.9742\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0728 - acc: 0.9732\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0914 - acc: 0.9654\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0658 - acc: 0.9768\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0713 - acc: 0.9728\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2]\n",
      "[1 2 1 1 3 2 2 2 2 1 1 1 2 2 2 2 1 2 2 2 3 1 2 0 4 3 3 1 2 4 3 1 1 1 1 3 2\n",
      " 2 0 1 1 4 3 1 1 3 1 2 2 2 1 2 2 1 1 1 1 1 2 1 0 2 1 2 3 3 2 1 3 1 3 2 1 1\n",
      " 1 1 0 1 1 1 1 1 2 1 3 1 2 2 1 1 1 3 1 1 1 1 1 0 1 2 2 2 2 1 1 2 1 2 3 1 1\n",
      " 2 1 1 3 3 1 1 2 0 2 3 1 3 1 1 0 1 1 2 1 1 2 1 2 1 1 1 2 1 2 1 0 2 3 2 1 2\n",
      " 3 3 3 1 2 1 1 1 1 0 3 1 2 2 1 1 1 2 2 1 1 2 2 1 1 2 2 1 1 0 1 1 2 1 1 2 2\n",
      " 2 1 2 1 0 1 2 3 3 2 2 2 2 1 1 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 4 3 1 1 2 3 2\n",
      " 2 3 4 1 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 2 3 2 1 1 2 1 2 2 1 3 2 2 3 1 1 3 1\n",
      " 1 0 1 1 1 3 1 1 2 1 2 1 1 2 4 2 1 2 1 1 1 1 1 3 1 3 2 2 1 1 1 3 4 2 2 1 1\n",
      " 1 2 2 1 1 2 2 3 3 0 0 1 1 3 2 0 2 1 1 1 1 1 1 2 3 1 2 1 1 1 2 3 2 1 1 1 2\n",
      " 1 1 1 1 0 2 2 1 4 1 3 1 3 3 2 2 1 2 3 1 2 0 2 2 2 3 2 1 3 1 4 2 0 3 1 2 1\n",
      " 1 1 2 3 4 4 0 2 2 2 1 2 2 3 0 3 1 2 3 1 0 2 1 2 1 1 3 2 2 1 0 2 1 1 2 1 1\n",
      " 2 2 2 1 1 1 1 0 1 2 0 3 3 1 1 1 2 1 2 2 3 1 2 0 1 1 3 3 3 1 2 3 1 3 1 1 3\n",
      " 1 1 3 4 1 2 3 3 1 1 1 1 2 1 1 2 2 1 4 1 1 1 1 3 2 1 4 0 4 3 1 3 1 2 2 2 1\n",
      " 4 2 2]\n",
      "acc: 67.98%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.array([t2mtest,rhtest,wstest,invtest,wtest,usheartest])\n",
    "#X_test=np.array([rhtest,wstest,invtest,wtest,usheartest,aotest,eutest])\n",
    "X_test.shape\n",
    "\n",
    "X_train=np.array([t2mtrain,rhtrain,wstrain,invtrain,wtrain,usheartrain])\n",
    "#X_train=np.array([rhtrain,wstrain,invtrain,wtrain,usheartrain,aotrain,eutrain])\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_reshape = np.einsum('lkija->klija',X_train)\n",
    "X_train_reshape.shape\n",
    "\n",
    "X_test_reshape = np.einsum('lkija->klija',X_test)\n",
    "X_test_reshape.shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16, kernel_size=3, activation='relu',padding='same',\n",
    "                     input_shape=(X_train_reshape.shape[1],X_train_reshape.shape[2],X_train_reshape.shape[3],1)),)\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Conv3D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(64, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(128, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=5, activation = 'softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics='acc')\n",
    "model.summary()\n",
    "history = model.fit(X_train_reshape, y_train, epochs=100)\n",
    "#yy_test = model.predict(X_test_reshape)\n",
    "yy_test=np.argmax ( model.predict ( X_test_reshape/255 ), axis=-1 )\n",
    "print(yy_test)\n",
    "y_test2=np.argmax (  y_test/255 , axis=-1 )\n",
    "print(y_test2)    \n",
    "plt.plot(yy_test,y_test2,'+')\n",
    "scores = model.evaluate(X_test_reshape, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1934, 18, 26, 1)\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_50 (Conv3D)           (None, 6, 18, 26, 16)     448       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_37 (Averag (None, 3, 9, 13, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_51 (Conv3D)           (None, 3, 9, 13, 32)      13856     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_38 (Averag (None, 2, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 2, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_52 (Conv3D)           (None, 2, 5, 7, 64)       55360     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_39 (Averag (None, 1, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_53 (Conv3D)           (None, 1, 3, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_40 (Averag (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_54 (Conv3D)           (None, 1, 2, 2, 256)      884992    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_41 (Averag (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,184,357\n",
      "Trainable params: 1,184,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 1.3803 - acc: 0.4201 - val_loss: 1.2782 - val_acc: 0.4649\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.3227 - acc: 0.4406 - val_loss: 1.2700 - val_acc: 0.4649\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2760 - acc: 0.4562 - val_loss: 1.2855 - val_acc: 0.4649\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2860 - acc: 0.4374 - val_loss: 1.2685 - val_acc: 0.4649\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.3024 - acc: 0.4441 - val_loss: 1.2737 - val_acc: 0.4649\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.2920 - acc: 0.4372 - val_loss: 1.2684 - val_acc: 0.4649\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2772 - acc: 0.4389 - val_loss: 1.2722 - val_acc: 0.4649\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 1.2892 - acc: 0.4239 - val_loss: 1.2291 - val_acc: 0.4669\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.2493 - acc: 0.4586 - val_loss: 1.1962 - val_acc: 0.5021\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.1706 - acc: 0.5087 - val_loss: 1.0878 - val_acc: 0.5289\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.1112 - acc: 0.5304 - val_loss: 1.0079 - val_acc: 0.5661\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 1.0494 - acc: 0.5706 - val_loss: 0.9966 - val_acc: 0.5764\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.9843 - acc: 0.5856 - val_loss: 0.9808 - val_acc: 0.5888\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.9663 - acc: 0.5760 - val_loss: 0.9577 - val_acc: 0.5620\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.9549 - acc: 0.5894 - val_loss: 0.9161 - val_acc: 0.6136\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.8759 - acc: 0.6177 - val_loss: 0.9371 - val_acc: 0.6322\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.8757 - acc: 0.6305 - val_loss: 0.9145 - val_acc: 0.6012\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.8820 - acc: 0.6293 - val_loss: 0.8919 - val_acc: 0.6343\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.8042 - acc: 0.6465 - val_loss: 0.8896 - val_acc: 0.6302\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.8108 - acc: 0.6490 - val_loss: 0.8727 - val_acc: 0.6240\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.7455 - acc: 0.6857 - val_loss: 0.9532 - val_acc: 0.6033\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.7388 - acc: 0.6954 - val_loss: 0.9081 - val_acc: 0.6240\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.7159 - acc: 0.6814 - val_loss: 0.8868 - val_acc: 0.6426\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.6762 - acc: 0.7094 - val_loss: 0.8309 - val_acc: 0.6384\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.6673 - acc: 0.7160 - val_loss: 0.8588 - val_acc: 0.6426\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.6337 - acc: 0.7231 - val_loss: 0.8675 - val_acc: 0.6591\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.5876 - acc: 0.7548 - val_loss: 0.8402 - val_acc: 0.6550\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.5840 - acc: 0.7540 - val_loss: 0.8858 - val_acc: 0.6529\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.5534 - acc: 0.7725 - val_loss: 0.8804 - val_acc: 0.6426\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.5451 - acc: 0.7734 - val_loss: 0.8874 - val_acc: 0.6632\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.5038 - acc: 0.7781 - val_loss: 0.9399 - val_acc: 0.6405\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.4921 - acc: 0.7958 - val_loss: 0.9369 - val_acc: 0.6446\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.4855 - acc: 0.8036 - val_loss: 1.0058 - val_acc: 0.6426\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.4484 - acc: 0.8117 - val_loss: 1.0204 - val_acc: 0.6384\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.4192 - acc: 0.8195 - val_loss: 0.9023 - val_acc: 0.6694\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.4062 - acc: 0.8197 - val_loss: 0.9573 - val_acc: 0.6591\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.3698 - acc: 0.8610 - val_loss: 1.0125 - val_acc: 0.6508\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.4043 - acc: 0.8279 - val_loss: 1.0446 - val_acc: 0.6798\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.3657 - acc: 0.8490 - val_loss: 1.0564 - val_acc: 0.6467\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.3201 - acc: 0.8593 - val_loss: 1.0203 - val_acc: 0.6529\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.3503 - acc: 0.8688 - val_loss: 1.0826 - val_acc: 0.6488\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.3448 - acc: 0.8548 - val_loss: 1.1749 - val_acc: 0.6446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.3278 - acc: 0.8658 - val_loss: 1.1394 - val_acc: 0.6364\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.2369 - acc: 0.9081 - val_loss: 1.2115 - val_acc: 0.6446\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.2548 - acc: 0.9036 - val_loss: 1.1980 - val_acc: 0.6384\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.2579 - acc: 0.8951 - val_loss: 1.1894 - val_acc: 0.6364\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.2328 - acc: 0.9181 - val_loss: 1.1748 - val_acc: 0.6488\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.2505 - acc: 0.8918 - val_loss: 1.1899 - val_acc: 0.6591\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.2228 - acc: 0.9123 - val_loss: 1.4134 - val_acc: 0.6694\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 1s 25ms/step - loss: 0.2483 - acc: 0.8996 - val_loss: 1.2872 - val_acc: 0.6488\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1915 - acc: 0.9312 - val_loss: 1.2407 - val_acc: 0.6570\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1815 - acc: 0.9337 - val_loss: 1.3492 - val_acc: 0.6570\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.2169 - acc: 0.9193 - val_loss: 1.3931 - val_acc: 0.6715\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1666 - acc: 0.9321 - val_loss: 1.2814 - val_acc: 0.6674\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1822 - acc: 0.9346 - val_loss: 1.5026 - val_acc: 0.6632\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.2018 - acc: 0.9273 - val_loss: 1.3466 - val_acc: 0.6632\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1900 - acc: 0.9309 - val_loss: 1.2652 - val_acc: 0.6488\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.1700 - acc: 0.9367 - val_loss: 1.3822 - val_acc: 0.6777\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.1793 - acc: 0.9362 - val_loss: 1.4188 - val_acc: 0.6839\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1559 - acc: 0.9360 - val_loss: 1.4439 - val_acc: 0.6694\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.1651 - acc: 0.9316 - val_loss: 1.4250 - val_acc: 0.6529\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1498 - acc: 0.9437 - val_loss: 1.5098 - val_acc: 0.6281\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.1770 - acc: 0.9346 - val_loss: 1.4550 - val_acc: 0.6777\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1332 - acc: 0.9524 - val_loss: 1.5935 - val_acc: 0.6612\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1425 - acc: 0.9476 - val_loss: 1.5476 - val_acc: 0.6736\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1242 - acc: 0.9496 - val_loss: 1.4915 - val_acc: 0.6694\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.1349 - acc: 0.9470 - val_loss: 1.5570 - val_acc: 0.6591\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1417 - acc: 0.9426 - val_loss: 1.5365 - val_acc: 0.6653\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1416 - acc: 0.9490 - val_loss: 1.4874 - val_acc: 0.6694\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1838 - acc: 0.9344 - val_loss: 1.5136 - val_acc: 0.6612\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1203 - acc: 0.9587 - val_loss: 1.5200 - val_acc: 0.6860\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1316 - acc: 0.9496 - val_loss: 1.6103 - val_acc: 0.6529\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1431 - acc: 0.9442 - val_loss: 1.5329 - val_acc: 0.6818\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1234 - acc: 0.9530 - val_loss: 1.5820 - val_acc: 0.6715\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1244 - acc: 0.9531 - val_loss: 1.5239 - val_acc: 0.6715\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1155 - acc: 0.9557 - val_loss: 1.7283 - val_acc: 0.6756\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1350 - acc: 0.9427 - val_loss: 1.5212 - val_acc: 0.6818\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0952 - acc: 0.9646 - val_loss: 1.6596 - val_acc: 0.6818\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1190 - acc: 0.9584 - val_loss: 1.5387 - val_acc: 0.6798\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1338 - acc: 0.9508 - val_loss: 1.5992 - val_acc: 0.6715\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1072 - acc: 0.9648 - val_loss: 1.6798 - val_acc: 0.6860\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0980 - acc: 0.9635 - val_loss: 1.6244 - val_acc: 0.6798\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1158 - acc: 0.9613 - val_loss: 1.4952 - val_acc: 0.6798\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0878 - acc: 0.9715 - val_loss: 1.6287 - val_acc: 0.6818\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1165 - acc: 0.9576 - val_loss: 1.6280 - val_acc: 0.6632\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0797 - acc: 0.9692 - val_loss: 1.7780 - val_acc: 0.6488\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1187 - acc: 0.9605 - val_loss: 1.7352 - val_acc: 0.6756\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0969 - acc: 0.9695 - val_loss: 1.7149 - val_acc: 0.6529\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0962 - acc: 0.9666 - val_loss: 1.7982 - val_acc: 0.6694\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0908 - acc: 0.9684 - val_loss: 1.7144 - val_acc: 0.6570\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0934 - acc: 0.9651 - val_loss: 1.6600 - val_acc: 0.6550\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.1030 - acc: 0.9621 - val_loss: 1.5383 - val_acc: 0.6570\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0950 - acc: 0.9604 - val_loss: 1.6621 - val_acc: 0.6901\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.1070 - acc: 0.9569 - val_loss: 1.7396 - val_acc: 0.6736\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0623 - acc: 0.9813 - val_loss: 1.6545 - val_acc: 0.6694\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0888 - acc: 0.9673 - val_loss: 1.8114 - val_acc: 0.6632\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0659 - acc: 0.9813 - val_loss: 1.8018 - val_acc: 0.6612\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0932 - acc: 0.9680 - val_loss: 1.7836 - val_acc: 0.6674\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0794 - acc: 0.9733 - val_loss: 1.7928 - val_acc: 0.6674\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0698 - acc: 0.9697 - val_loss: 1.7602 - val_acc: 0.6653\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3]\n",
      "[1 2 1 1 3 2 2 2 2 1 1 1 2 2 2 2 1 2 2 2 3 1 2 0 4 3 3 1 2 4 3 1 1 1 1 3 2\n",
      " 2 0 1 1 4 3 1 1 3 1 2 2 2 1 2 2 1 1 1 1 1 2 1 0 2 1 2 3 3 2 1 3 1 3 2 1 1\n",
      " 1 1 0 1 1 1 1 1 2 1 3 1 2 2 1 1 1 3 1 1 1 1 1 0 1 2 2 2 2 1 1 2 1 2 3 1 1\n",
      " 2 1 1 3 3 1 1 2 0 2 3 1 3 1 1 0 1 1 2 1 1 2 1 2 1 1 1 2 1 2 1 0 2 3 2 1 2\n",
      " 3 3 3 1 2 1 1 1 1 0 3 1 2 2 1 1 1 2 2 1 1 2 2 1 1 2 2 1 1 0 1 1 2 1 1 2 2\n",
      " 2 1 2 1 0 1 2 3 3 2 2 2 2 1 1 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 4 3 1 1 2 3 2\n",
      " 2 3 4 1 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 2 3 2 1 1 2 1 2 2 1 3 2 2 3 1 1 3 1\n",
      " 1 0 1 1 1 3 1 1 2 1 2 1 1 2 4 2 1 2 1 1 1 1 1 3 1 3 2 2 1 1 1 3 4 2 2 1 1\n",
      " 1 2 2 1 1 2 2 3 3 0 0 1 1 3 2 0 2 1 1 1 1 1 1 2 3 1 2 1 1 1 2 3 2 1 1 1 2\n",
      " 1 1 1 1 0 2 2 1 4 1 3 1 3 3 2 2 1 2 3 1 2 0 2 2 2 3 2 1 3 1 4 2 0 3 1 2 1\n",
      " 1 1 2 3 4 4 0 2 2 2 1 2 2 3 0 3 1 2 3 1 0 2 1 2 1 1 3 2 2 1 0 2 1 1 2 1 1\n",
      " 2 2 2 1 1 1 1 0 1 2 0 3 3 1 1 1 2 1 2 2 3 1 2 0 1 1 3 3 3 1 2 3 1 3 1 1 3\n",
      " 1 1 3 4 1 2 3 3 1 1 1 1 2 1 1 2 2 1 4 1 1 1 1 3 2 1 4 0 4 3 1 3 1 2 2 2 1\n",
      " 4 2 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 66.53%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAART0lEQVR4nO3df6jdd33H8edrWURZu0XIne2S3N6NlW0qay13scUxinNbf7HC8I8Ks1AYoaWOCo6ROaj4x6BjIKNWGsKUrUwmA6t0JsXJZlFhrSYxSVujW9S6hgYaKyaGilvce3+cr+Ps5Jzc78k9995zPzwfcMj3x+d+z/tzP5dXv/dzv6efVBWSpM3vpza6AEnSbBjoktQIA12SGmGgS1IjDHRJasRPb9Qbb9++vZaWljbq7SVpUzp8+PB3q2ph3LkNC/SlpSUOHTq0UW8vSZtSku9MOueUiyQ1wkCXpEYY6JLUCANdkhphoEtSI3oHepItSb6a5DNjziXJw0lOJjme5IbZlimtr6W9Bza6BGlq09yhPwCcmHDuVuDa7rUHeHSVdUmSptQr0JPsBG4H/mZCkzuBx2rgaWBbkqtnVKMkqYe+Hyz6a+BPgSsnnN8BvDi0f6o7dnq4UZI9DO7gWVxcnKZOac2NTrMM77/w0O3rXY40tRUDPckdwMtVdTjJzZOajTl20coZVbUf2A+wvLzsyhqaK8OhvbT3gCGuTafPlMvbgN9P8gLwCeDtSf5+pM0pYNfQ/k7gpZlUKEnqZcVAr6o/q6qdVbUE3AX8a1X94UizJ4C7u6ddbgTOVtXp0WtJktbOZf/PuZLcC1BV+4CDwG3ASeBV4J6ZVCdtEKdbtBlNFehV9RTwVLe9b+h4AffPsjBJ0nT8pKgkNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRErBnqS1yb5cpJjSZ5P8sExbW5OcjbJ0e714NqUK0mapM+KRT8C3l5V55NsBb6U5Mmqenqk3Rer6o7ZlyhJ6mPFQO+Wlzvf7W7tXrWWRUmSptdrDj3JliRHgZeBz1XVM2Oa3dRNyzyZ5E0TrrMnyaEkh86cOXP5VUuSLtIr0Kvqx1V1PbAT2J3kzSNNjgDXVNV1wIeBT0+4zv6qWq6q5YWFhcuvWpJ0kamecqmq7wNPAbeMHD9XVee77YPA1iTbZ1SjJKmHPk+5LCTZ1m2/DngH8PWRNlclSbe9u7vuKzOvVpI0UZ+nXK4G/i7JFgZB/Y9V9Zkk9wJU1T7gncB9SS4APwTu6v6YKklaJ32ecjkOvGXM8X1D248Aj8y2NEnSNPykqCQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEX2WoHttki8nOZbk+SQfHNMmSR5OcjLJ8SQ3rE250vpY2ntgo0uQptZnCbofAW+vqvNJtgJfSvJkVT091OZW4Nru9Vbg0e5fSdI6WfEOvQbOd7tbu9foeqF3Ao91bZ8GtiW5eralSpIupc8dOt0C0YeBXwY+UlXPjDTZAbw4tH+qO3Z65Dp7gD0Ai4uLl1mytDZGp1mG91946Pb1LkeaWq9Ar6ofA9cn2QZ8Ksmbq+q5oSYZ92VjrrMf2A+wvLx80XlpIw2H9tLeA4a4Np2pnnKpqu8DTwG3jJw6Bewa2t8JvLSawiRJ0+nzlMtCd2dOktcB7wC+PtLsCeDu7mmXG4GzVXUaSdK66TPlcjXwd908+k8B/1hVn0lyL0BV7QMOArcBJ4FXgXvWqF5pXTjdos1oxUCvquPAW8Yc3ze0XcD9sy1NkjQNPykqSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSI/qsWLQryeeTnEjyfJIHxrS5OcnZJEe714NrU64kaZI+KxZdAN5XVUeSXAkcTvK5qvraSLsvVtUdsy9RktTHinfoVXW6qo502z8ATgA71rowSdJ0pppDT7LEYDm6Z8acvinJsSRPJnnThK/fk+RQkkNnzpyZvlpJ0kS9Az3JFcAngfdW1bmR00eAa6rqOuDDwKfHXaOq9lfVclUtLywsXGbJkqRxegV6kq0MwvzjVfX46PmqOldV57vtg8DWJNtnWqkk6ZL6POUS4KPAiar60IQ2V3XtSLK7u+4rsyxUknRpfZ5yeRvwbuDZJEe7Y+8HFgGqah/wTuC+JBeAHwJ3VVXNvlxJ0iQrBnpVfQnICm0eAR6ZVVGSpOn5SVFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiP6LEG3K8nnk5xI8nySB8a0SZKHk5xMcjzJDWtTrrQ+lvYe2OgSpKn1WYLuAvC+qjqS5ErgcJLPVdXXhtrcClzbvd4KPNr9K0laJyveoVfV6ao60m3/ADgB7BhpdifwWA08DWxLcvXMq5UkTdTnDv3/JFkC3gI8M3JqB/Di0P6p7tjpka/fA+wBWFxcnLJUaW2NTrMM77/w0O3rXY40td6BnuQK4JPAe6vq3OjpMV9SFx2o2g/sB1heXr7ovLSRhkN7ae8BQ1ybTq+nXJJsZRDmH6+qx8c0OQXsGtrfCby0+vIkSX31ecolwEeBE1X1oQnNngDu7p52uRE4W1WnJ7SVJK2BPlMubwPeDTyb5Gh37P3AIkBV7QMOArcBJ4FXgXtmXqm0jpxu0Wa0YqBX1ZcYP0c+3KaA+2dVlCRpen5SVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiD5L0H0syctJnptw/uYkZ5Mc7V4Pzr5MSdJK+ixB97fAI8Bjl2jzxaq6YyYVSZIuy4p36FX1BeB761CLJGkVZjWHflOSY0meTPKmSY2S7ElyKMmhM2fOzOitJUkwm0A/AlxTVdcBHwY+PalhVe2vquWqWl5YWJjBW0uSfmLVgV5V56rqfLd9ENiaZPuqK5MkTWXVgZ7kqiTptnd313xltdeVJE1nxadckvwDcDOwPckp4APAVoCq2ge8E7gvyQXgh8BdVVVrVrEkaawVA72q3rXC+UcYPNYoSdpAflJUkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRqwY6Ek+luTlJM9NOJ8kDyc5meR4khtmX6a0vpb2HtjoEqSp9blD/1vglkucvxW4tnvtAR5dfVmSpGmtGOhV9QXge5docifwWA08DWxLcvWsCpQk9bPiEnQ97ABeHNo/1R07PdowyR4Gd/EsLi7O4K2l2RmdZhnef+Gh29e7HGlqswj0jDk2dpHoqtoP7AdYXl52IWnNleHQXtp7wBDXpjOLp1xOAbuG9ncCL83gupKkKcwi0J8A7u6edrkROFtVF023SJLW1opTLkn+AbgZ2J7kFPABYCtAVe0DDgK3ASeBV4F71qpYab043aLNaMVAr6p3rXC+gPtnVpEk6bL4SVFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiN6BXqSW5J8I8nJJHvHnL85ydkkR7vXg7MvVZJ0KX2WoNsCfAT4HQYLQn8lyRNV9bWRpl+sqjvWoEZJUg997tB3Ayer6ltV9V/AJ4A717YsSdK0+gT6DuDFof1T3bFRNyU5luTJJG8ad6Eke5IcSnLozJkzl1GuJGmSPoGeMcdqZP8IcE1VXQd8GPj0uAtV1f6qWq6q5YWFhakKlSRdWp9APwXsGtrfCbw03KCqzlXV+W77ILA1yfaZVSlJWlGfQP8KcG2SX0zyGuAu4InhBkmuSpJue3d33VdmXawkabIVn3KpqgtJ3gN8FtgCfKyqnk9yb3d+H/BO4L4kF4AfAndV1ei0jCRpDWWjcnd5ebkOHTq0Ie8tSZtVksNVtTzunJ8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1olegJ7klyTeSnEyyd8z5JHm4O388yQ2zL1VaP0t7D2x0CdLUVgz0JFuAjwC3Am8E3pXkjSPNbgWu7V57gEdnXKckaQV97tB3Ayer6ltV9V/AJ4A7R9rcCTxWA08D25JcPeNaJUmXsOIi0cAO4MWh/VPAW3u02QGcHm6UZA+DO3gWFxenrVVaU6PTLMP7Lzx0+3qXI02tT6BnzLHRlaX7tKGq9gP7YbBIdI/3ltbNcGgv7T1giGvT6TPlcgrYNbS/E3jpMtpIktZQn0D/CnBtkl9M8hrgLuCJkTZPAHd3T7vcCJytqtOjF5IkrZ0Vp1yq6kKS9wCfBbYAH6uq55Pc253fBxwEbgNOAq8C96xdydLac7pFm1GfOXSq6iCD0B4+tm9ou4D7Z1uaJGkaflJUkhphoEtSIwx0SWqEgS5Jjcjg75kb8MbJGeA7l/nl24HvzrCcjWRf5lMrfWmlH2BffuKaqloYd2LDAn01khyqquWNrmMW7Mt8aqUvrfQD7EsfTrlIUiMMdElqxGYN9P0bXcAM2Zf51EpfWukH2JcVbco5dEnSxTbrHbokaYSBLkmNmKtAT7IryeeTnEjyfJIHxrT5uST/lORY1+aeoXMvJHk2ydEkh9a3+ovq7NOX1yf5VLew9peTvHno3CUX5l4vM+jHPI3Ja7v6fvKz88ExbSYueD4vY9LVstq+bLZx+dUk/5bkR0n+ZOTcXIzLDPqx+jGpqrl5AVcDN3TbVwL/DrxxpM37gb/stheA7wGv6fZfALZvdD+m6MtfAR/otn8V+JduewvwTeCXgNcAx0a/djP0Yw7HJMAV3fZW4BngxpE2twFPdm1vBJ6ZtzFZbV826bj8PPAbwF8AfzJ0fG7GZTX9mNWYzNUdelWdrqoj3fYPgBMM1ib9f82AK5MEuIJBoF9Y10J76NmXNwL/0rX5OrCU5A30W5h7XayyH3OlBs53u1u71+hTAZMWPJ+bMYFV92Wu9OlLVb1cVV8B/nvky+dmXFbZj5mYq0AflmQJeAuD/8oNewT4NQZL3D0LPFBV/9OdK+CfkxzOYEHquXCJvhwD/qBrsxu4hsHyfZMW3d5Ql9EPmLMxSbIlyVHgZeBzVTXal0nf+7kbk1X0BTbfuEwyV+Oyin7ADMZkLgM9yRXAJ4H3VtW5kdO/BxwFfgG4Hngkyc92595WVTcAtwL3J/mt9al4shX68hDw+u4H4I+BrzL4baPXotvr6TL7AXM2JlX146q6nsF/cHYPz/d3Jn3v525MVtEX2HzjMslcjcsq+gEzGJO5C/QkWxkEx8er6vExTe4BHu9+vTkJfJvBvC1V9VL378vApxj8OrZhVupLVZ2rqnu6H4C7GfxN4NvM2aLbq+jH3I3JT1TV94GngFtGTk363s/VmAy7jL5sxnGZZC7H5TL6MZMxmatA7+bFPwqcqKoPTWj2n8Bvd+3fAPwK8K0kP5Pkyu74zwC/Czy39lWP16cvSbZlsPA2wB8BX+jufvsszL0uVtOPORyThSTbuu3XAe8Avj7SbNKC53MzJrC6vmzScZlkbsZlNf2Y2Zis5i+qs34Bv8ng16XjDKZVjjL4S/29wL1dm18A/pnB/PlzwB92x3+JwVzuMeB54M83QV9uAv6jG/THgdcPff1tDJ4o+eZG9mU1/ZjDMfl1BtNBx7ufnQe748N9CfCR7vv+LLA8b2Oy2r5s0nG5isHd+Dng+932z87TuKymH7MaEz/6L0mNmKspF0nS5TPQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiP+F0CoRBdNdXf/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.array([t2mtest,rhtest,wstest,invtest,wtest,usheartest])\n",
    "#X_test=np.array([rhtest,wstest,invtest,wtest,usheartest,aotest,eutest])\n",
    "X_test.shape\n",
    "\n",
    "X_train=np.array([t2mtrain,rhtrain,wstrain,invtrain,wtrain,usheartrain])\n",
    "#X_train=np.array([rhtrain,wstrain,invtrain,wtrain,usheartrain,aotrain,eutrain])\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_reshape = np.einsum('lkija->klija',X_train)\n",
    "X_train_reshape.shape\n",
    "\n",
    "X_test_reshape = np.einsum('lkija->klija',X_test)\n",
    "X_test_reshape.shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16, kernel_size=3, activation='relu',padding='same',\n",
    "                     input_shape=(X_train_reshape.shape[1],X_train_reshape.shape[2],X_train_reshape.shape[3],1)),)\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Conv3D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(64, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(128, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(256, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=5, activation = 'softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics='acc')\n",
    "model.summary()\n",
    "history = model.fit(X_train_reshape, y_train, validation_data=(X_test_reshape, y_test),epochs=100)\n",
    "#yy_test = model.predict(X_test_reshape)\n",
    "yy_test=np.argmax ( model.predict ( X_test_reshape/255 ), axis=-1 )\n",
    "print(yy_test)\n",
    "y_test2=np.argmax (  y_test/255 , axis=-1 )\n",
    "print(y_test2)    \n",
    "plt.plot(yy_test,y_test2,'+')\n",
    "scores = model.evaluate(X_test_reshape, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1934, 18, 26, 1)\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_76 (Conv3D)           (None, 9, 18, 26, 16)     448       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_63 (Averag (None, 5, 9, 13, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_77 (Conv3D)           (None, 5, 9, 13, 32)      13856     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_64 (Averag (None, 3, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 3, 5, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_78 (Conv3D)           (None, 3, 5, 7, 64)       55360     \n",
      "_________________________________________________________________\n",
      "average_pooling3d_65 (Averag (None, 2, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 2, 3, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_79 (Conv3D)           (None, 2, 3, 4, 128)      221312    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_66 (Averag (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1, 2, 2, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_80 (Conv3D)           (None, 1, 2, 2, 256)      884992    \n",
      "_________________________________________________________________\n",
      "average_pooling3d_67 (Averag (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 1,966,501\n",
      "Trainable params: 1,966,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 3s 38ms/step - loss: 1.4054 - acc: 0.4199 - val_loss: 1.2807 - val_acc: 0.4649\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2771 - acc: 0.4453 - val_loss: 1.2815 - val_acc: 0.4649\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2818 - acc: 0.4467 - val_loss: 1.2745 - val_acc: 0.4649\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 2s 34ms/step - loss: 1.2757 - acc: 0.4591 - val_loss: 1.2769 - val_acc: 0.4649\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 2s 34ms/step - loss: 1.2975 - acc: 0.4387 - val_loss: 1.2685 - val_acc: 0.4649\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 2s 36ms/step - loss: 1.3184 - acc: 0.4313 - val_loss: 1.2803 - val_acc: 0.4649\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 2s 36ms/step - loss: 1.2796 - acc: 0.4387 - val_loss: 1.2700 - val_acc: 0.4649\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 2s 40ms/step - loss: 1.2743 - acc: 0.4508 - val_loss: 1.2740 - val_acc: 0.4649\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 1.2717 - acc: 0.4499 - val_loss: 1.2745 - val_acc: 0.4649\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 1.3084 - acc: 0.4379 - val_loss: 1.2730 - val_acc: 0.4649\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 2s 34ms/step - loss: 1.2982 - acc: 0.4430 - val_loss: 1.2730 - val_acc: 0.4649\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 1.2938 - acc: 0.4431 - val_loss: 1.2723 - val_acc: 0.4649\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2963 - acc: 0.4425 - val_loss: 1.3021 - val_acc: 0.4298\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2720 - acc: 0.4576 - val_loss: 1.2549 - val_acc: 0.4752\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 1.2483 - acc: 0.4798 - val_loss: 1.2806 - val_acc: 0.4607\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 1.2582 - acc: 0.4581 - val_loss: 1.2636 - val_acc: 0.4793\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2586 - acc: 0.4693 - val_loss: 1.2565 - val_acc: 0.4711\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 1.2374 - acc: 0.4642 - val_loss: 1.2703 - val_acc: 0.4545\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2441 - acc: 0.4800 - val_loss: 1.2380 - val_acc: 0.4711\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.2249 - acc: 0.4779 - val_loss: 1.2792 - val_acc: 0.4566\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[1 2 1 1 3 2 2 2 2 1 1 1 2 2 2 2 1 2 2 2 3 1 2 0 4 3 3 1 2 4 3 1 1 1 1 3 2\n",
      " 2 0 1 1 4 3 1 1 3 1 2 2 2 1 2 2 1 1 1 1 1 2 1 0 2 1 2 3 3 2 1 3 1 3 2 1 1\n",
      " 1 1 0 1 1 1 1 1 2 1 3 1 2 2 1 1 1 3 1 1 1 1 1 0 1 2 2 2 2 1 1 2 1 2 3 1 1\n",
      " 2 1 1 3 3 1 1 2 0 2 3 1 3 1 1 0 1 1 2 1 1 2 1 2 1 1 1 2 1 2 1 0 2 3 2 1 2\n",
      " 3 3 3 1 2 1 1 1 1 0 3 1 2 2 1 1 1 2 2 1 1 2 2 1 1 2 2 1 1 0 1 1 2 1 1 2 2\n",
      " 2 1 2 1 0 1 2 3 3 2 2 2 2 1 1 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 4 3 1 1 2 3 2\n",
      " 2 3 4 1 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 2 3 2 1 1 2 1 2 2 1 3 2 2 3 1 1 3 1\n",
      " 1 0 1 1 1 3 1 1 2 1 2 1 1 2 4 2 1 2 1 1 1 1 1 3 1 3 2 2 1 1 1 3 4 2 2 1 1\n",
      " 1 2 2 1 1 2 2 3 3 0 0 1 1 3 2 0 2 1 1 1 1 1 1 2 3 1 2 1 1 1 2 3 2 1 1 1 2\n",
      " 1 1 1 1 0 2 2 1 4 1 3 1 3 3 2 2 1 2 3 1 2 0 2 2 2 3 2 1 3 1 4 2 0 3 1 2 1\n",
      " 1 1 2 3 4 4 0 2 2 2 1 2 2 3 0 3 1 2 3 1 0 2 1 2 1 1 3 2 2 1 0 2 1 1 2 1 1\n",
      " 2 2 2 1 1 1 1 0 1 2 0 3 3 1 1 1 2 1 2 2 3 1 2 0 1 1 3 3 3 1 2 3 1 3 1 1 3\n",
      " 1 1 3 4 1 2 3 3 1 1 1 1 2 1 1 2 2 1 4 1 1 1 1 3 2 1 4 0 4 3 1 3 1 2 2 2 1\n",
      " 4 2 2]\n",
      "acc: 45.66%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.array([t2mtest,rhtest,wstest,invtest,wtest,usheartest,AO5Dtest,EU5Dtest,SST30Dtest])\n",
    "#X_test=np.array([rhtest,wstest,invtest,wtest,usheartest,aotest,eutest])\n",
    "X_test.shape\n",
    "\n",
    "X_train=np.array([t2mtrain,rhtrain,wstrain,invtrain,wtrain,usheartrain,AO5Dtrain,EU5Dtrain,SST30Dtrain])\n",
    "#X_train=np.array([rhtrain,wstrain,invtrain,wtrain,usheartrain,aotrain,eutrain])\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_reshape = np.einsum('lkija->klija',X_train)\n",
    "X_train_reshape.shape\n",
    "\n",
    "X_test_reshape = np.einsum('lkija->klija',X_test)\n",
    "X_test_reshape.shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16, kernel_size=3, activation='relu',padding='same',\n",
    "                     input_shape=(X_train_reshape.shape[1],X_train_reshape.shape[2],X_train_reshape.shape[3],1)),)\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Conv3D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(64, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(128, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv3D(256, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(units=5, activation = 'softmax'))\n",
    "\n",
    "adam = keras.optimizers.RMSprop(lr=0.1)#(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics='acc')\n",
    "model.summary()\n",
    "history = model.fit(X_train_reshape, y_train, validation_data=(X_test_reshape, y_test),epochs=20)\n",
    "#yy_test = model.predict(X_test_reshape)\n",
    "yy_test=np.argmax ( model.predict ( X_test_reshape/255 ), axis=-1 )\n",
    "print(yy_test)\n",
    "y_test2=np.argmax (  y_test/255 , axis=-1 )\n",
    "print(y_test2)    \n",
    "plt.plot(yy_test,y_test2,'+')\n",
    "scores = model.evaluate(X_test_reshape, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1934, 18, 26, 1)\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_70 (Conv3D)           (None, 3, 18, 26, 16)     448       \n",
      "_________________________________________________________________\n",
      "average_pooling3d_57 (Averag (None, 2, 9, 13, 16)      0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 3744)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 5)                 18725     \n",
      "=================================================================\n",
      "Total params: 19,173\n",
      "Trainable params: 19,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "61/61 [==============================] - 1s 10ms/step - loss: 1.3757 - acc: 0.4022 - val_loss: 1.3126 - val_acc: 0.4607\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 0s 8ms/step - loss: 1.3154 - acc: 0.4371 - val_loss: 1.2815 - val_acc: 0.4504\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2522 - acc: 0.4692 - val_loss: 1.2974 - val_acc: 0.4298\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2854 - acc: 0.4154 - val_loss: 1.2986 - val_acc: 0.4649\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2696 - acc: 0.4313 - val_loss: 1.2851 - val_acc: 0.4649\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2766 - acc: 0.4556 - val_loss: 1.2716 - val_acc: 0.4545\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2729 - acc: 0.4370 - val_loss: 1.2744 - val_acc: 0.4566\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2960 - acc: 0.4415 - val_loss: 1.2668 - val_acc: 0.4587\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2414 - acc: 0.4567 - val_loss: 1.2725 - val_acc: 0.4649\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.2687 - acc: 0.4552 - val_loss: 1.2812 - val_acc: 0.4525\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[1 2 1 1 3 2 2 2 2 1 1 1 2 2 2 2 1 2 2 2 3 1 2 0 4 3 3 1 2 4 3 1 1 1 1 3 2\n",
      " 2 0 1 1 4 3 1 1 3 1 2 2 2 1 2 2 1 1 1 1 1 2 1 0 2 1 2 3 3 2 1 3 1 3 2 1 1\n",
      " 1 1 0 1 1 1 1 1 2 1 3 1 2 2 1 1 1 3 1 1 1 1 1 0 1 2 2 2 2 1 1 2 1 2 3 1 1\n",
      " 2 1 1 3 3 1 1 2 0 2 3 1 3 1 1 0 1 1 2 1 1 2 1 2 1 1 1 2 1 2 1 0 2 3 2 1 2\n",
      " 3 3 3 1 2 1 1 1 1 0 3 1 2 2 1 1 1 2 2 1 1 2 2 1 1 2 2 1 1 0 1 1 2 1 1 2 2\n",
      " 2 1 2 1 0 1 2 3 3 2 2 2 2 1 1 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 4 3 1 1 2 3 2\n",
      " 2 3 4 1 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 2 3 2 1 1 2 1 2 2 1 3 2 2 3 1 1 3 1\n",
      " 1 0 1 1 1 3 1 1 2 1 2 1 1 2 4 2 1 2 1 1 1 1 1 3 1 3 2 2 1 1 1 3 4 2 2 1 1\n",
      " 1 2 2 1 1 2 2 3 3 0 0 1 1 3 2 0 2 1 1 1 1 1 1 2 3 1 2 1 1 1 2 3 2 1 1 1 2\n",
      " 1 1 1 1 0 2 2 1 4 1 3 1 3 3 2 2 1 2 3 1 2 0 2 2 2 3 2 1 3 1 4 2 0 3 1 2 1\n",
      " 1 1 2 3 4 4 0 2 2 2 1 2 2 3 0 3 1 2 3 1 0 2 1 2 1 1 3 2 2 1 0 2 1 1 2 1 1\n",
      " 2 2 2 1 1 1 1 0 1 2 0 3 3 1 1 1 2 1 2 2 3 1 2 0 1 1 3 3 3 1 2 3 1 3 1 1 3\n",
      " 1 1 3 4 1 2 3 3 1 1 1 1 2 1 1 2 2 1 4 1 1 1 1 3 2 1 4 0 4 3 1 3 1 2 2 2 1\n",
      " 4 2 2]\n",
      "acc: 45.25%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ+UlEQVR4nO3df4xlZX3H8fdHXAL+yprsptDdHUZT0lZMEDJBKElD1Kb8Skka/sBESUnNBoMGWxOz+gfE/0jaGAMYNhulSrQa468Qdok1KgH+AN1dAYHVZqtYtmzDinGRQjRrv/3jHpuby9ydc2fuvcM8vF/JyZ5znmfO/T53Zj8cnjl3n1QVkqSN7zXrXYAkaToMdElqhIEuSY0w0CWpEQa6JDXitev1wlu2bKnFxcX1enlJ2pAOHDjwy6raulzbugX64uIi+/fvX6+Xl6QNKckvxrU55SJJjTDQJakRBrokNcJAl6RGGOiS1IjegZ7klCQ/SnLPMm1JcmuSw0keS3L+dMuU5mtx1971LkGa2CR36DcCh8a0XQac3W07gTvWWJckaUK9Aj3JduAK4LNjulwF3FUDDwGbk5w5pRolST30/WDRp4GPAW8c074NeHro+Eh37uhwpyQ7GdzBs7CwMEmd0syNTrMMHz91yxXzLkea2IqBnuRK4NmqOpDkknHdljn3spUzqmoPsAdgaWnJlTX0ijIc2ou79hri2nD6TLlcDPxNkqeArwDvSvLFkT5HgB1Dx9uBZ6ZSoSSplxUDvao+XlXbq2oRuAb4XlW9b6Tb3cC13dMuFwLHq+ro6LUkSbOz6n+cK8n1AFW1G9gHXA4cBl4ErptKddI6cbpFG9FEgV5V9wH3dfu7h84XcMM0C5MkTcZPikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGrFioCc5LckPkjya5Ikkn1ymzyVJjid5pNtumk25kqRx+qxY9FvgXVX1QpJNwINJ7q2qh0b6PVBVV06/RElSHysGere83Avd4aZuq1kWJUmaXK859CSnJHkEeBb4TlU9vEy3i7ppmXuTnDPmOjuT7E+y/9ixY6uvWpL0Mr0Cvap+X1XvALYDFyR5+0iXg8BZVXUucBvwrTHX2VNVS1W1tHXr1tVXLUl6mYmecqmqXwP3AZeOnH++ql7o9vcBm5JsmVKNkqQe+jzlsjXJ5m7/dOA9wE9G+pyRJN3+Bd11n5t6tZKksfo85XIm8IUkpzAI6q9W1T1Jrgeoqt3A1cAHk5wAXgKu6X6ZKkmakz5PuTwGnLfM+d1D+7cDt0+3NEnSJPykqCQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEX2WoDstyQ+SPJrkiSSfXKZPktya5HCSx5KcP5typflY3LV3vUuQJtZnCbrfAu+qqheSbAIeTHJvVT001Ocy4OxueydwR/enJGlOVrxDr4EXusNN3Ta6XuhVwF1d34eAzUnOnG6pkqST6XOHTrdA9AHgT4DPVNXDI122AU8PHR/pzh0duc5OYCfAwsLCKkuWZmN0mmX4+Klbrph3OdLEegV6Vf0eeEeSzcA3k7y9qh4f6pLlvmyZ6+wB9gAsLS29rF1aT8OhvbhrryGuDWeip1yq6tfAfcClI01HgB1Dx9uBZ9ZSmCRpMn2ectna3ZmT5HTgPcBPRrrdDVzbPe1yIXC8qo4iSZqbPlMuZwJf6ObRXwN8taruSXI9QFXtBvYBlwOHgReB62ZUrzQXTrdoI1ox0KvqMeC8Zc7vHtov4IbpliZJmoSfFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRfVYs2pHk+0kOJXkiyY3L9LkkyfEkj3TbTbMpV5I0Tp8Vi04AH62qg0neCBxI8p2qenKk3wNVdeX0S5Qk9bHiHXpVHa2qg93+b4BDwLZZFyZJmsxEc+hJFhksR/fwMs0XJXk0yb1Jzhnz9TuT7E+y/9ixY5NXK0kaq3egJ3kD8HXgI1X1/EjzQeCsqjoXuA341nLXqKo9VbVUVUtbt25dZcmSpOX0CvQkmxiE+Zeq6huj7VX1fFW90O3vAzYl2TLVSiVJJ9XnKZcAnwMOVdWnxvQ5o+tHkgu66z43zUIlSSfX5ymXi4H3Az9O8kh37hPAAkBV7QauBj6Y5ATwEnBNVdX0y5UkjbNioFfVg0BW6HM7cPu0ipIkTc5PikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGtFnCbodSb6f5FCSJ5LcuEyfJLk1yeEkjyU5fzblSvOxuGvvepcgTazPEnQngI9W1cEkbwQOJPlOVT051Ocy4OxueydwR/enJGlOVrxDr6qjVXWw2/8NcAjYNtLtKuCuGngI2JzkzKlXK0kaq88d+v9LsgicBzw80rQNeHro+Eh37ujI1+8EdgIsLCxMWKo0W6PTLMPHT91yxbzLkSbWO9CTvAH4OvCRqnp+tHmZL6mXnajaA+wBWFpaelm7tJ6GQ3tx115DXBtOr6dckmxiEOZfqqpvLNPlCLBj6Hg78Mzay5Mk9dXnKZcAnwMOVdWnxnS7G7i2e9rlQuB4VR0d01eSNAN9plwuBt4P/DjJI925TwALAFW1G9gHXA4cBl4Erpt6pdIcOd2ijWjFQK+qB1l+jny4TwE3TKsoSdLk/KSoJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRfZaguzPJs0keH9N+SZLjSR7ptpumX6YkaSV9lqD7PHA7cNdJ+jxQVVdOpSJJ0qqseIdeVfcDv5pDLZKkNZjWHPpFSR5Ncm+Sc8Z1SrIzyf4k+48dOzall5YkwXQC/SBwVlWdC9wGfGtcx6raU1VLVbW0devWKby0JOkP1hzoVfV8Vb3Q7e8DNiXZsubKJEkTWXOgJzkjSbr9C7prPrfW60qSJrPiUy5JvgxcAmxJcgS4GdgEUFW7gauBDyY5AbwEXFNVNbOKJUnLWjHQq+q9K7TfzuCxRknSOvKTopLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDVixUBPcmeSZ5M8PqY9SW5NcjjJY0nOn36Z0nwt7tq73iVIE+tzh/554NKTtF8GnN1tO4E71l6WJGlSKwZ6Vd0P/OokXa4C7qqBh4DNSc6cVoGSpH5WXIKuh23A00PHR7pzR0c7JtnJ4C6ehYWFKby0ND2j0yzDx0/dcsW8y5EmNo1AzzLnll0kuqr2AHsAlpaWXEharyjDob24a68hrg1nGk+5HAF2DB1vB56ZwnUlSROYRqDfDVzbPe1yIXC8ql423SJJmq0Vp1ySfBm4BNiS5AhwM7AJoKp2A/uAy4HDwIvAdbMqVpoXp1u0Ea0Y6FX13hXaC7hhahVJklbFT4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhrRK9CTXJrkp0kOJ9m1TPslSY4neaTbbpp+qZKkk+mzBN0pwGeAv2KwIPQPk9xdVU+OdH2gqq6cQY2SpB763KFfAByuqp9V1e+ArwBXzbYsSdKk+gT6NuDpoeMj3blRFyV5NMm9Sc5Z7kJJdibZn2T/sWPHVlGuJGmcPoGeZc7VyPFB4KyqOhe4DfjWcheqqj1VtVRVS1u3bp2oUEnSyfUJ9CPAjqHj7cAzwx2q6vmqeqHb3wdsSrJlalVKklbUJ9B/CJyd5C1JTgWuAe4e7pDkjCTp9i/orvvctIuVJI234lMuVXUiyYeAbwOnAHdW1RNJru/adwNXAx9McgJ4CbimqkanZSRJM5T1yt2lpaXav3//ury2JG1USQ5U1dJybX5SVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiF6BnuTSJD9NcjjJrmXak+TWrv2xJOdPv1RpfhZ37V3vEqSJrRjoSU4BPgNcBrwNeG+St410uww4u9t2AndMuU5J0gr63KFfAByuqp9V1e+ArwBXjfS5CrirBh4CNic5c8q1SpJOYsVFooFtwNNDx0eAd/bosw04OtwpyU4Gd/AsLCxMWqs0U6PTLMPHT91yxbzLkSbWJ9CzzLnRlaX79KGq9gB7YLBIdI/XluZmOLQXd+01xLXh9JlyOQLsGDreDjyzij6SpBnqE+g/BM5O8pYkpwLXAHeP9LkbuLZ72uVC4HhVHR29kCRpdlaccqmqE0k+BHwbOAW4s6qeSHJ9174b2AdcDhwGXgSum13J0uw53aKNqM8cOlW1j0FoD5/bPbRfwA3TLU2SNAk/KSpJjTDQJakRBrokNcJAl6RGZPD7zHV44eQY8It1efG12QL8cr2LmDPH3L5X23hh4475rKraulzDugX6RpVkf1UtrXcd8+SY2/dqGy+0OWanXCSpEQa6JDXCQJ/cnvUuYB045va92sYLDY7ZOXRJaoR36JLUCANdkhphoHd6LIT95iTf7BbB/kGStw+1bU7ytSQ/SXIoyUXzrX511jjmf0jyRJLHk3w5yWnzrX51ktyZ5Nkkj49pH7vg+Urv1yvRasebZEeS73c/z08kuXG+la/eWr7HXfspSX6U5J75VDxFVfWq3xj8s8D/AbwVOBV4FHjbSJ9/Am7u9v8M+O5Q2xeAD3T7pwKb13tMsxwzg+UFfw6c3h1/Ffi79R5Tz3H/JXA+8PiY9suBexmswnUh8HDf9+uVuK1hvGcC53f7bwT+fSOMdy1jHmr/R+BfgXvWeyyTbt6hD/RZCPttwHcBquonwGKSP0ryJgY/QJ/r2n5XVb+eW+Wrt+oxd22vBU5P8lrgdWyQFaqq6n7gVyfpMm7B8z7v1yvOasdbVUer6mB3jd8Ahxj8h/wVbw3fY5JsB64APjv7SqfPQB8Yt8j1sEeBvwVIcgFwFoOl9t4KHAP+pfvftM8mef3sS16zVY+5qv4L+GfgPxksBH68qv5t5hXPx7j3pc/7tRGtOK4ki8B5wMPzK2umTjbmTwMfA/53zjVNhYE+0GeR61uANyd5BPgw8CPgBIM71fOBO6rqPOB/gI0wv7rqMSd5M4O7nLcAfwy8Psn7ZljrPI17X3othL4BnXRcSd4AfB34SFU9P7eqZmvZMSe5Eni2qg7Mu6Bp6bVi0avAiotcdz/M18HglyoM5pB/zmC64UhV/eHu5WtsjEBfy5j/Gvh5VR3r2r4B/AXwxdmXPXPj3pdTx5zf6Mb+HCTZxCDMv1RV31iH2mZl3JivBv4myeXAacCbknyxqjbMzYp36AMrLoTdPclyanf4AeD+qnq+qv4beDrJn3Zt7waenFfha7DqMTOYarkwyeu6oH83gznWFoxb8LzPYukb0bLj7b6vnwMOVdWn1rfEqVt2zFX18araXlWLDL6/39tIYQ7eoQO9F8L+c+CuJL9nENh/P3SJDwNf6v6i/4wNsEj2WsZcVQ8n+RpwkMG004/YIB+jTvJl4BJgS5IjwM3AJjj5gufj3q+5D2BCqx0vcDHwfuDH3ZQbwCdqsL7wK9oaxrzh+dF/SWqEUy6S1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXi/wACfdGFuOS7UQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test=np.array([AO5Dtest,EU5Dtest,SST30Dtest])\n",
    "#X_test=np.array([rhtest,wstest,invtest,wtest,usheartest,aotest,eutest])\n",
    "X_test.shape\n",
    "\n",
    "X_train=np.array([AO5Dtrain,EU5Dtrain,SST30Dtrain])\n",
    "#X_train=np.array([rhtrain,wstrain,invtrain,wtrain,usheartrain,aotrain,eutrain])\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_reshape = np.einsum('lkija->klija',X_train)\n",
    "X_train_reshape.shape\n",
    "\n",
    "X_test_reshape = np.einsum('lkija->klija',X_test)\n",
    "X_test_reshape.shape\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16, kernel_size=3, activation='relu',padding='same',\n",
    "                     input_shape=(X_train_reshape.shape[1],X_train_reshape.shape[2],X_train_reshape.shape[3],1)),)\n",
    "model.add(AveragePooling3D(pool_size=2,padding='same'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=5, activation = 'sigmoid'))\n",
    "\n",
    "adam = keras.optimizers.RMSprop(lr=0.1)#(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics='acc')\n",
    "model.summary()\n",
    "history = model.fit(X_train_reshape, y_train, validation_data=(X_test_reshape, y_test),epochs=10)\n",
    "#yy_test = model.predict(X_test_reshape)\n",
    "yy_test=np.argmax ( model.predict ( X_test_reshape/255 ), axis=-1 )\n",
    "print(yy_test)\n",
    "y_test2=np.argmax (  y_test/255 , axis=-1 )\n",
    "print(y_test2)    \n",
    "plt.plot(yy_test,y_test2,'+')\n",
    "scores = model.evaluate(X_test_reshape, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
