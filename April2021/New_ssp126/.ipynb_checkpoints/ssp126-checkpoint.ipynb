{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv3D, Flatten,MaxPooling3D,AveragePooling3D, concatenate,Input ,SpatialDropout3D,Dropout\n",
    "import keras\n",
    "from math import e\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Orography\n",
    "OroData = xr.open_dataset('../../../Data/eraDown/ERA5_2degree_Down/DailyMean/ERA5IGP_Orography.nc')\n",
    "model = load_model('../../March2021/Observation_models/modelCNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['ACCESS-CM2']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "        #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    y   = load('../../../March2021/Observation_models/ObsY.joblib') \n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IITM-ESM\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['IITM-ESM']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "        #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanESM5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-c821fe562e0a>:55: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-26-c821fe562e0a>:67: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-26-c821fe562e0a>:124: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = t2m.indexes['time'].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['CanESM5']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_cent_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_cent_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_cent_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_cent_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_cent_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_cent_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_cent_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_cent_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_cent_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    t2m['time'] = datetimeindex\n",
    "    ws['time'] = datetimeindex\n",
    "    rh['time'] = datetimeindex\n",
    "    inv['time'] = datetimeindex\n",
    "    w['time'] = datetimeindex\n",
    "    ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INM-CM4-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-cce8b733cc77>:55: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-28-cce8b733cc77>:67: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-28-cce8b733cc77>:124: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = t2m.indexes['time'].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['INM-CM4-8']\n",
    "gridlist=['gr1']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    t2m['time'] = datetimeindex\n",
    "    ws['time'] = datetimeindex\n",
    "    rh['time'] = datetimeindex\n",
    "    inv['time'] = datetimeindex\n",
    "    w['time'] = datetimeindex\n",
    "    ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INM-CM5-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-d90cde920d69>:55: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-29-d90cde920d69>:67: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
      "<ipython-input-29-d90cde920d69>:124: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetimeindex = t2m.indexes['time'].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['INM-CM5-0']\n",
    "gridlist=['gr1']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    t2m['time'] = datetimeindex\n",
    "    ws['time'] = datetimeindex\n",
    "    rh['time'] = datetimeindex\n",
    "    inv['time'] = datetimeindex\n",
    "    w['time'] = datetimeindex\n",
    "    ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC-Earth3\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['EC-Earth3']\n",
    "gridlist=['gr']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPSL-CM6A-LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['IPSL-CM6A-LR']\n",
    "gridlist=['gr']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIROC6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "<ipython-input-39-c6ccf51c3c6f>:231: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "einstein sum subscripts string contains too many subscripts for operand 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c6ccf51c3c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mX_reshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lkija->klija'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mX_reshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: einstein sum subscripts string contains too many subscripts for operand 0"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['MIROC6']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['MPI-ESM1-2-HR']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-LR\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['MPI-ESM1-2-LR']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRI-ESM2-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/cccr/diptih/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:687: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs, dtype=np.float64)\n"
     ]
    }
   ],
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['MRI-ESM2-0']\n",
    "gridlist=['gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_cen_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_cen_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_cen_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_cen_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_cen_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_cen_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_cen_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_cen_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_cen_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "    #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    #datetimeindex = t2m.indexes['time'].to_datetimeindex()\n",
    "    #t2m['time'] = datetimeindex\n",
    "    #ws['time'] = datetimeindex\n",
    "    #rh['time'] = datetimeindex\n",
    "    #inv['time'] = datetimeindex\n",
    "    #w['time'] = datetimeindex\n",
    "    #ushear['time'] = datetimeindex\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'_ssp126.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Read Data \n",
    "#ModelList=['ACCESS-CM2','CanESM5','IITM-ESM','INM-CM4-8','INM-CM5-0',\n",
    " #          'IPSL-CM6A-LR','MIROC6','MRI-ESM2-0','MPI-ESM1-2-LR','MPI-ESM1-2-HR','EC-Earth3']\n",
    "#varList=['hurs','ta','tas','ua','uas','vas','wap','zg']\n",
    "ModelList=['ACCESS-CM2','INM-CM4-8','INM-CM5-0','IPSL-CM6A-LR','IITM-ESM']\n",
    "gridlist=['gn','gr1','gr1','gr','gn']\n",
    "\n",
    "for m,g in zip(ModelList,gridlist) :\n",
    "    print(m)\n",
    "    folderString='/home/cccr/diptih/dipti/Data/ssp126/'+m+'/processed/'\n",
    "    #print(folderString)\n",
    "    #fName=folderString+'Regrid_'+v+'_day_'+m+'_ssp126_r1i1p1f1_gn_20150101-21001231.nc'\n",
    "    t2mData=xr.open_dataset(folderString+'Regrid_tas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(t2mData)\n",
    "    rhData=xr.open_dataset(folderString+'Regrid_hurs_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(rhData)\n",
    "    v10Data=xr.open_dataset(folderString+'Regrid_vas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(v10Data)\n",
    "    u10Data=xr.open_dataset(folderString+'Regrid_uas_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(u10Data)\n",
    "    tLevData=xr.open_dataset(folderString+'Regrid_ta_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(tLevData)\n",
    "    zLevData=xr.open_dataset(folderString+'Regrid_zg_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(zLevData)\n",
    "    wLevData=xr.open_dataset(folderString+'Regrid_wap_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(wLevData)\n",
    "    uLevData=xr.open_dataset(folderString+'Regrid_ua_day_'+m+'_ssp126_r1i1p1f1_'+g+'_20150101-21001231.nc')\n",
    "    #print(uLevData)\n",
    "    # Calculate wind speed and relative humidity inv  ushear\n",
    "    ws = ((v10Data.vas.values**2)+(u10Data.uas.values**2))**0.5\n",
    "    ws_ds = xr.Dataset({'ws': (('time','latitude','longitude'), ws)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    rh_ds = xr.Dataset({'rh': (('time','latitude','longitude'), rhData.hurs/100.0)},\n",
    "                   coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "\n",
    "    #Calculate inv\n",
    "    inv=t2mData.tas.values-tLevData.ta.sel(plev=85000,method='nearest').values\n",
    "    inv_ds = xr.Dataset({'inv': (('time','latitude','longitude'), inv)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    inv_ds.attrs\n",
    "    inv_ds.attrs['units']='K'\n",
    "    inv_ds.attrs['long_name']='t2m - t850'\n",
    "\n",
    "    #u shear calculation\n",
    "    ushear=(uLevData.ua.sel(plev=85000,method='nearest').values-u10Data.uas.values)/(zLevData.zg.sel(plev=85000,method='nearest').values) \n",
    "    ushear_ds = xr.Dataset({'ushear': (('time','latitude','longitude'), ushear)}, coords={'time': v10Data.time,'latitude': v10Data.latitude,'longitude': v10Data.longitude})\n",
    "    ushear_ds.attrs['units']='s-1'\n",
    "    ushear_ds.attrs['long_name']='(u10 - u850)/z850'\n",
    "    \n",
    "\n",
    "\n",
    "    # AO data\n",
    "    AOData = xr.open_dataset(m+'-AOindex-NDJF-Daily-2015-2100.nc')\n",
    "    aoTS=AOData.AO\n",
    "        #datetimeindex = aoTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #aoTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((AOData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(aoTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), aoTS[t].values)\n",
    "    AOData=xr.Dataset({'AO': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': aoTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude}) \n",
    "    # EU data\n",
    "    EUData = xr.open_dataset(m+'-EUindex-NDJF-Daily-2015-2100.nc')\n",
    "    EUData.EU\n",
    "    euTS=EUData.EU\n",
    "    #datetimeindex = euTS.indexes['time'].to_datetimeindex()\n",
    "    #datetimeindex\n",
    "    #euTS['time'] = datetimeindex\n",
    "    Darray=np.zeros((EUData.time.shape[0],t2mData.latitude.shape[0], t2mData.longitude.shape[0]))\n",
    "    for t in range(euTS.time.shape[0]) :\n",
    "        Darray[t,:,:]=np.full((t2mData.latitude.shape[0], t2mData.longitude.shape[0]), euTS[t].values)\n",
    "    EUData=xr.Dataset({'EU': (('time','latitude','longitude'), Darray)},\n",
    "                  coords={'time': euTS.time,'latitude': t2mData.latitude,'longitude': t2mData.longitude})\n",
    "    # create mask\n",
    "    oro = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100))\n",
    "    oro.values = OroData.z.sel(latitude=slice(35,0),longitude=slice(50,100)).values/9.81\n",
    "    oro.attrs\n",
    "    oro.attrs['units']='meter'\n",
    "    oro.attrs['long_name']='Orography'\n",
    "    oro.values[oro.values>500.1]=np.NaN\n",
    "    mask=oro.values/oro.values\n",
    "    #AO\n",
    "    AO5D=AOData.AO.rolling(time=5).mean()\n",
    "\n",
    "    AO5DAll=AO5D[((AO5D.time.dt.month>11) | (AO5D.time.dt.month<2)) & \n",
    "             (AO5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "\n",
    "    #EU\n",
    "    EU5D=EUData.EU.rolling(time=5).mean()\n",
    "\n",
    "    EU5DAll=EU5D[((EU5D.time.dt.month>11) | (EU5D.time.dt.month<2)) & \n",
    "             (EU5D.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),\n",
    "                                           longitude=slice(50,100))\n",
    "    \n",
    "    t1=AO5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    AO5DAll.values=t1.unstack()\n",
    "\n",
    "    t1=EU5DAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    EU5DAll.values=t1.unstack()\n",
    "\n",
    "    AO5DAll.values=AO5DAll.values*mask\n",
    "    AO5DAll.values=xr.where(np.isnan(AO5DAll.values),  0.000000000001,AO5DAll.values)\n",
    "\n",
    "    EU5DAll.values=EU5DAll.values*mask\n",
    "    EU5DAll.values=xr.where(np.isnan(EU5DAll.values),  0.000000000001,EU5DAll.values)\n",
    "    \n",
    "    t2m=t2mData.tas.shift(time=1)\n",
    "    ws=ws_ds.ws.shift(time=1)\n",
    "    rh=rh_ds.rh.shift(time=1)\n",
    "    inv=inv_ds.inv.shift(time=1)\n",
    "    w=wLevData.wap.sel(plev=70000,method='nearest').shift(time=1)\n",
    "    ushear=ushear_ds.ushear.shift(time=1)\n",
    "    \n",
    "    t2mTsAll=t2m[((t2m.time.dt.month>11) | (t2m.time.dt.month<2)) & (t2m.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wsTsAll=ws[((ws.time.dt.month>11) | (ws.time.dt.month<2)) & (ws.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    rhTsAll=rh[((rh.time.dt.month>11) | (rh.time.dt.month<2)) & (rh.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    invTsAll=inv[((inv.time.dt.month>11) | (inv.time.dt.month<2)) & (inv.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    ushearTsAll=ushear[((ushear.time.dt.month>11) | (ushear.time.dt.month<2)) & (ushear.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    wTsAll=w[((w.time.dt.month>11) | (w.time.dt.month<2)) & (w.time.dt.year<2101)].sel(time=slice('2015-1-1','2100-12-31'),latitude=slice(35,0),longitude=slice(50,100))\n",
    "    \n",
    "    \n",
    "    t1=t2mTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    t2mTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=wsTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wsTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=rhTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    rhTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=invTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    invTsAll.values=t1.unstack()\n",
    "\n",
    "    t1=ushearTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    ushearTsAll.values=t1.unstack()\n",
    "\n",
    "\n",
    "    t1=wTsAll.stack(z=(\"latitude\", \"longitude\"))\n",
    "    # fit scaler on training data\n",
    "    norm = StandardScaler().fit(t1)\n",
    "    # transform training data\n",
    "    t1.values = norm.transform(t1)\n",
    "    wTsAll.values=t1.unstack()\n",
    "    \n",
    "    t2mTsAll.values=t2mTsAll.values*mask\n",
    "    wsTsAll.values=wsTsAll.values*mask\n",
    "    rhTsAll.values=rhTsAll.values*mask\n",
    "    invTsAll.values=invTsAll.values*mask\n",
    "    ushearTsAll.values=ushearTsAll.values*mask\n",
    "    wTsAll.values=wTsAll.values*mask\n",
    "    \n",
    "    t2mTsAll.values=xr.where(np.isnan(t2mTsAll.values),  0.000000000001,t2mTsAll.values)\n",
    "    wsTsAll.values=xr.where(np.isnan(wsTsAll.values),  0.000000000001,wsTsAll.values)\n",
    "    rhTsAll.values=xr.where(np.isnan(rhTsAll.values),  0.000000000001,rhTsAll.values)\n",
    "    invTsAll.values=xr.where(np.isnan(invTsAll.values),  0.000000000001,invTsAll.values)\n",
    "    ushearTsAll.values=xr.where(np.isnan(ushearTsAll.values),  0.000000000001,ushearTsAll.values)\n",
    "    wTsAll.values=xr.where(np.isnan(wTsAll.values),  0.000000000001,wTsAll.values)\n",
    "    \n",
    "    t2mt=t2mTsAll.values\n",
    "    t2mt=t2mt[:,:,:,None]\n",
    "    t2mt.shape\n",
    "\n",
    "\n",
    "    wst=wsTsAll.values\n",
    "    wst=wst[:,:,:,None]\n",
    "    wst.shape\n",
    "\n",
    "    rht=rhTsAll.values\n",
    "    rht=rht[:,:,:,None]\n",
    "    rht.shape\n",
    "\n",
    "\n",
    "    invt=invTsAll.values\n",
    "    invt=invt[:,:,:,None]\n",
    "    invt.shape\n",
    "\n",
    "    wt=wTsAll.values\n",
    "    wt=wt[:,:,:,None]\n",
    "    wt.shape\n",
    "\n",
    "    usheart=ushearTsAll.values\n",
    "    usheart=usheart[:,:,:,None]\n",
    "    usheart.shape\n",
    "\n",
    "    aot=AO5DAll.values\n",
    "    aot=aot[:,:,:,None]\n",
    "    aot.shape\n",
    "\n",
    "    eut=EU5DAll.values\n",
    "    eut=eut[:,:,:,None]\n",
    "    eut.shape\n",
    "    \n",
    "    X=np.array([t2mt,rht,wst,invt,wt,usheart,aot,eut])\n",
    "    X.shape\n",
    "    \n",
    "    X_reshape = np.einsum('lkija->klija',X)\n",
    "    X_reshape.shape\n",
    "    \n",
    "    yLR=model.predict(X_reshape)\n",
    "    y_predLin_ds=xr.Dataset({'yLR': (('time'), yLR[:,0])}, coords={'time':t2mTsAll.time.values})\n",
    "    \n",
    "    dump(y_predLin_ds.yLR,'Modelplots_future/'+m+'.joblib')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
